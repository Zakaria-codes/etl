# ğŸš€ ETL Pipeline Project

![Python](https://img.shields.io/badge/Python-3.x-blue)
![Status](https://img.shields.io/badge/Status-Active-success)
![License](https://img.shields.io/badge/License-MIT-green)
![Maintained](https://img.shields.io/badge/Maintained-Yes-brightgreen)

An automated **ETL (Extract, Transform, Load)** pipeline written in
Python.\
This project demonstrates how to extract raw data, clean and transform
it, then load it into a structured destination such as a database or
file.

This repository is ideal for: - Data engineering students\
- ETL beginners\
- Anyone building automated data pipelines

------------------------------------------------------------------------

## ğŸ“ Project Structure

    etl/
    â”‚
    â”œâ”€â”€ DATASETS/               # Input / output datasets
    â”‚
    â”œâ”€â”€ main.py                 # Main ETL orchestrator
    â”œâ”€â”€ extraction.py           # Extract logic
    â”œâ”€â”€ transformation.py       # Data cleaning & transformation
    â”œâ”€â”€ loading.py              # Load logic (files or DB)
    â”‚
    â”œâ”€â”€ dbconfig.py             # Database connection config
    â”œâ”€â”€ requirements.txt        # Dependencies
    â””â”€â”€ README.md              

------------------------------------------------------------------------

## ğŸ§  ETL Workflow

### **1. Extract**

-   Reads data from CSV/JSON/API/Database (depending on your pipeline
    setup)
-   Located in: `extraction.py`

### **2. Transform**

-   Cleans missing values\
-   Standardizes formats\
-   Applies feature transformations\
-   Located in: `transformation.py`

### **3. Load**

-   Saves cleaned data into:
    -   CSV files\
    -   Databases (MySQL, PostgreSQL, etc.)\
-   Located in: `loading.py`

------------------------------------------------------------------------

## âš™ï¸ Installation & Setup

### **1. Clone the repository**

``` bash
git clone https://github.com/Zakaria-codes/etl
cd etl
```

### **2. (Optional) Create a virtual environment**

``` bash
python -m venv venv
# Activate:
# Windows:
venv\Scriptsctivate
# Linux/macOS:
source venv/bin/activate
```

### **3. Install dependencies**

``` bash
pip install -r requirements.txt
```

------------------------------------------------------------------------

## ğŸš€ Running the ETL Pipeline

Run the entire pipeline:

``` bash
python main.py
```

Or run only one step (if you prefer manual debugging):

### Extract only

``` bash
python extraction.py
```

### Transform only

``` bash
python transformation.py
```

### Load only

``` bash
python loading.py
```

------------------------------------------------------------------------

## ğŸ§ª Example Workflow

``` bash
python main.py --input DATASETS/source.csv --output DATASETS/cleaned_output.csv
```

------------------------------------------------------------------------

## ğŸ“¦ Dependencies

Key libraries used:

-   pandas
-   numpy
-   sqlalchemy
-   mysql-connector-python
-   python-dotenv

------------------------------------------------------------------------

## ğŸ› ï¸ Configuration

Modify your DB settings in:

    dbconfig.py

------------------------------------------------------------------------

## ğŸ§© Features

âœ” Extracts datasets from multiple sources\
âœ” Cleans and transforms data automatically\
âœ” Loads the final dataset into files or DB\
âœ” Modular structure\
âœ” Easy to extend

------------------------------------------------------------------------

## ğŸ§­ Roadmap

-   [ ] Add API extraction\
-   [ ] Add logging\
-   [ ] Add Airflow scheduling\
-   [ ] Add unit tests\
-   [ ] Add Docker support

------------------------------------------------------------------------

## ğŸ¤ Contributing

1.  Fork repository\
2.  Create branch\
3.  Commit changes\
4.  Push\
5.  Open PR

